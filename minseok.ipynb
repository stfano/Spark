{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a2abdc-3572-4d1a-b6b2-512d0283ef4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/26 13:39:12 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "# -- spark 열기 --\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "# -- sesion 열기 --\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"myAppName\")\n",
    "    .config(\"spark.executor.instances\", \"6\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \n",
    "    .config(\"spark.executor.memoryOverhead\", \"2g\")\n",
    "    .config(\"spark.driver.memoryOverhead\", \"2g\")\n",
    "    .config(\"spark.master\", \"yarn\")\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84cd5c4-4242-44a6-a585-37e57a08989f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# also_buy, also_viewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7b4827c-2fc1-46b0-915c-42d8427d2c51",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "from pyspark.sql.functions import input_file_name, regexp_extract\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"also_buy\", ArrayType(StringType()), True),\n",
    "    StructField(\"also_view\", ArrayType(StringType()), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37ae341c-e1e9-4fb3-8ac8-66bf2a69b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로 설정 (HDFS 내의 경로)\n",
    "file_path = \"/minipro/ama/meta\"\n",
    "\n",
    "# 모든 JSON 파일을 읽어서 하나의 DataFrame으로 결합\n",
    "df_relation = spark.read.schema(schema).json(file_path + \"/*.json\")\n",
    "\n",
    "# 필요한 경우, 'filename' 컬럼 제거\n",
    "df_relation = df_relation.drop(\"filename\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2228fdda-8f37-488e-88d8-8dae97a9cedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|      asin|    also_buy|           also_view|\n",
      "+----------+------------+--------------------+\n",
      "|0764443682|        null|                null|\n",
      "|1291691480|        null|                null|\n",
      "|1940280001|        null|                null|\n",
      "|1940735033|        null|                null|\n",
      "|1940967805|        null|                null|\n",
      "|1942705034|        null|                null|\n",
      "|3293015344|        null|                null|\n",
      "|5378828716|        null|                null|\n",
      "|6041002984|        null|                null|\n",
      "|630456984X|        null|                null|\n",
      "|7106116521|        null|                null|\n",
      "|8037200124|        null|                null|\n",
      "|8037200221|        null|                null|\n",
      "|8279996567|        null|                null|\n",
      "|9239282785|        null|                null|\n",
      "|9239281533|        null|                null|\n",
      "|9269808971|        null|                null|\n",
      "|9654263246|        null|[B07CQ3KY5B, B014...|\n",
      "|B00004T3SN|[B01KA5PTYG]|[B00Q5RRK78, B00R...|\n",
      "|B00005OTJ8|        null|                null|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 결과 DataFrame 출력\n",
    "df_relation.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "852047c3-8882-481a-9423-6b94ae087f04",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "# asin을 기준으로 explode를 적용하여 새로운 컬럼 생성\n",
    "df_exploded = df_relation.withColumn(\"also_buy\", explode(\"also_buy\"))\n",
    "df_also_buy = df_exploded.select(\"asin\", \"also_buy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a916f87d-726a-404c-9d26-ed88a4369f9c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_exploded2 = df_relation.withColumn(\"also_view\", explode(\"also_view\"))\n",
    "df_also_view = df_exploded2.select(\"asin\", \"also_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e61f5b3c-fa5a-47bf-a095-606e8c7d4347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      asin|  also_buy|\n",
      "+----------+----------+\n",
      "|B00004T3SN|B01KA5PTYG|\n",
      "|B00007GDFV|B07C9V84JD|\n",
      "|B00007GDFV|B01J6JE05G|\n",
      "|B00007GDFV|B07JJQFHS5|\n",
      "|B00007GDFV|B003EGITUK|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+----------+\n",
      "|      asin| also_view|\n",
      "+----------+----------+\n",
      "|9654263246|B07CQ3KY5B|\n",
      "|9654263246|B014TEOG3O|\n",
      "|9654263246|B078429G6J|\n",
      "|9654263246|B01FRG9Z70|\n",
      "|9654263246|B06W9MXMM3|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_also_buy.show(5), df_also_view.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f348f7d-bc6a-4df2-9320-6fe9319e0f28",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_also_buy.write.parquet(\"/minipro/parquet/df_also_buy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2ee2be4-4b61-4f83-8885-6e3519adc8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_also_view.write.parquet(\"/minipro/parquet/df_also_view.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9260b1-c95b-4e7f-85be-9de812c93171",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4d85d73-81d3-472a-b9d3-cd0dc7879206",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"rank\", StringType(), True),\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"price\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4bd72114-871f-49f7-9d40-c10e61924df6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 모든 파일을 읽어서 하나의 DataFrame으로 결합\n",
    "file_path = f\"/minipro/ama/meta/\"\n",
    "\n",
    "df_product = spark.read.schema(schema).json(file_path + \"/*.json\")\n",
    "\n",
    "# 파일 이름에서 카테고리 추출하여 컬럼 추가\n",
    "df_product = df_product.withColumn(\"filename\", input_file_name())\n",
    "df_product = df_product.withColumn(\"category\", regexp_extract(\"filename\", \"meta_(.*?)\\\\.json\", 1))\n",
    "\n",
    "# 필요한 경우, 'filename' 컬럼 제거\n",
    "df_product = df_product.drop(\"filename\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fdec9267-d53e-453b-8846-5dd267047b1c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+------+--------------+\n",
      "|               title|               brand|                rank|      asin| price|      category|\n",
      "+--------------------+--------------------+--------------------+----------+------+--------------+\n",
      "|Slime Time Fall F...|Group Publishing ...|13,052,976inCloth...|0764443682|  null|AMAZON_FASHION|\n",
      "|XCC Qi promise ne...|                null|11,654,581inCloth...|1291691480|  null|AMAZON_FASHION|\n",
      "|Magical Things I ...|   Christopher Manos|19,308,073inCloth...|1940280001|  null|AMAZON_FASHION|\n",
      "|Ashes to Ashes, O...|Flickerlamp Publi...|19,734,184inCloth...|1940735033|  null|AMAZON_FASHION|\n",
      "|Aether & Empire #...|                null|10,558,646inCloth...|1940967805| $4.50|AMAZON_FASHION|\n",
      "|365 Affirmations ...|                null|16,179,013inCloth...|1942705034|  null|AMAZON_FASHION|\n",
      "|Blessed by Pope B...|                null|7,787,039inClothi...|3293015344|  null|AMAZON_FASHION|\n",
      "|Womens Sexy Sleev...|              Didala|9,854,284inClothi...|5378828716|  null|AMAZON_FASHION|\n",
      "|Sevendayz Men's S...|           sevendayz|16,147,550inCloth...|6041002984|  null|AMAZON_FASHION|\n",
      "|Dante's Peak - La...|                null|16,714,561inCloth...|630456984X|  null|AMAZON_FASHION|\n",
      "|Milliongadgets(TM...|                null|2,081,098inClothi...|7106116521|  null|AMAZON_FASHION|\n",
      "|Envirosax Kids Se...|           Envirosax|25,698,912inCloth...|8037200124|  null|AMAZON_FASHION|\n",
      "|Envirosax Greengr...|           Envirosax|25,850,143inCloth...|8037200221|  null|AMAZON_FASHION|\n",
      "|Blessed by Pope B...|      Gifts by Lulee|19,411,377inCloth...|8279996567|  null|AMAZON_FASHION|\n",
      "|Tideclothes ALAGI...|         Tideclothes|21,572,343inCloth...|9239282785|  null|AMAZON_FASHION|\n",
      "|ALAGIRLS Straples...|         Tideclothes|22,202,218inCloth...|9239281533|  null|AMAZON_FASHION|\n",
      "|Syma S107C 3chann...|                null|9,900,066inClothi...|9269808971|  null|AMAZON_FASHION|\n",
      "|X. L. Carbon Fibe...|         Roar Carbon|3,725,957inClothi...|9654263246|$14.99|AMAZON_FASHION|\n",
      "|Shimmer Anne Shin...|  Shimmer Anne Shine|468,314inClothing...|B00004T3SN| $6.99|AMAZON_FASHION|\n",
      "|SpongeBob Squarep...|SpongeBob SquareP...|23,888,557inCloth...|B00005OTJ8|  null|AMAZON_FASHION|\n",
      "+--------------------+--------------------+--------------------+----------+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_product.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80024cd-d44b-4bd5-b94a-4e09e1f0b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_product.write.parquet(\"/minipro/parquet/df_product.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64238750-f1ea-432d-b829-e475bbee3050",
   "metadata": {},
   "source": [
    "# reviewer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "138e0565-c473-43d6-b36d-f769293c9b76",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "schema = StructType([\n",
    "    StructField(\"reviewerID\", StringType(), True),\n",
    "    StructField(\"reviewerName\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d4ccbf3-e24c-4a13-98ef-39c90f94990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로 설정 (HDFS 내의 경로)\n",
    "file_path = \"/minipro/ama\"\n",
    "\n",
    "# 모든 JSON 파일을 읽어서 하나의 DataFrame으로 결합\n",
    "df_reviewer_id = spark.read.schema(schema).json(file_path + \"/*.json\")\n",
    "\n",
    "# 필요한 경우, 'filename' 컬럼 제거\n",
    "df_reviewer_id = df_reviewer_id.drop(\"filename\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af74d859-e895-4a1b-aa6f-563eb06d504a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|    reviewerID|        reviewerName|\n",
      "+--------------+--------------------+\n",
      "|A1D4G1SNUZWQOT|               Tracy|\n",
      "|A3DDWDH9PX2YX2|           Sonja Lau|\n",
      "|A2MWC41EW7XL15|            Kathleen|\n",
      "|A2UH2QQ275NV45|         Jodi Stoner|\n",
      "| A89F3LQADZBS5|        Alexander D.|\n",
      "|A29HLOUW0NS0EH|   Patricia R. Erwin|\n",
      "| A7QS961ROI6E0|    REBECCA S LAYTON|\n",
      "|A1BB77SEBQT8VX|  Darrow H Ankrum II|\n",
      "| AHWOW7D1ABO9C|              rosieO|\n",
      "| AKS3GULZE0HFC|          M. Waltman|\n",
      "| A38NS6NF6WPXS|            BTDoxies|\n",
      "|A1KOKO3HTSAI1H|        Robin Howard|\n",
      "|A1G3S57JGZNPCL|kimberly a schott...|\n",
      "| AGBL3TTP6GV4X|             gallina|\n",
      "|A1Y36BSE9GKXLV|            Ms Irish|\n",
      "|A1L1U968VNYVA4|                J.G.|\n",
      "|A1NSKPSR0XZ0C9|               Jules|\n",
      "|A3O5SXH5O8DWRP|     Debra Humphreys|\n",
      "|A3I52T3ZCLRZZA|          Ann Bishop|\n",
      "|A3VWTJR1QOI7JR|     Amazon Customer|\n",
      "+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_reviewer_id.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "54c7bf26-0d93-4a94-8f05-0133b7510022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_reviewer_id.write.parquet(\"/minipro/parquet/df_reviewer_id.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee186d43-4b48-4a50-bc94-5aec05fbbff1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c1f28127-c670-4301-b3d6-a1b3149d669c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, FloatType, BooleanType, StructField, StringType, IntegerType, ArrayType\n",
    "schema = StructType([\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"overall\", FloatType(), True),\n",
    "    StructField(\"vote\", StringType(), True),\n",
    "    StructField(\"verified\", BooleanType(), True),\n",
    "    StructField(\"reviewText\", StringType(), True),\n",
    "    StructField(\"reviewTime\", StringType(), True),\n",
    "    StructField(\"reviewerID\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dff2ffdd-c003-4a8e-a7ae-05bc3bee87b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로 설정 (HDFS 내의 경로)\n",
    "file_path = \"/minipro/ama\"\n",
    "\n",
    "# 모든 JSON 파일을 읽어서 하나의 DataFrame으로 결합\n",
    "df_review = spark.read.schema(schema).json(file_path + \"/*.json\")\n",
    "\n",
    "# 필요한 경우, 'filename' 컬럼 제거\n",
    "df_review = df_review.drop(\"filename\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "905f574e-d73b-4307-a566-8f0875d94fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----+--------+--------------------+-----------+--------------+\n",
      "|      asin|overall|vote|verified|          reviewText| reviewTime|    reviewerID|\n",
      "+----------+-------+----+--------+--------------------+-----------+--------------+\n",
      "|7106116521|    5.0|null|    true|Exactly what I ne...|10 20, 2014|A1D4G1SNUZWQOT|\n",
      "|7106116521|    2.0|   3|    true|I agree with the ...|09 28, 2014|A3DDWDH9PX2YX2|\n",
      "|7106116521|    4.0|null|   false|Love these... I a...|08 25, 2014|A2MWC41EW7XL15|\n",
      "|7106116521|    2.0|null|    true| too tiny an opening|08 24, 2014|A2UH2QQ275NV45|\n",
      "|7106116521|    3.0|null|   false|                Okay|07 27, 2014| A89F3LQADZBS5|\n",
      "|7106116521|    5.0|null|    true|Exactly what I wa...|07 19, 2014|A29HLOUW0NS0EH|\n",
      "|7106116521|    4.0|null|    true|These little plas...|05 31, 2014| A7QS961ROI6E0|\n",
      "|B00007GDFV|    3.0|null|    true|mother - in - law...|09 22, 2013|A1BB77SEBQT8VX|\n",
      "|B00007GDFV|    3.0|null|    true|Item is of good q...|07 17, 2013| AHWOW7D1ABO9C|\n",
      "|B00007GDFV|    3.0|null|    true|I had used my las...|04 13, 2013| AKS3GULZE0HFC|\n",
      "|B00007GDFV|    4.0|null|    true|This brand has be...| 03 9, 2013| A38NS6NF6WPXS|\n",
      "|B00007GDFV|    2.0|null|    true|I smoke 100's and...|01 27, 2013|A1KOKO3HTSAI1H|\n",
      "|B00007GDFV|    1.0|null|    true|cheap and cheesy,...| 01 4, 2013|A1G3S57JGZNPCL|\n",
      "|B00007GDFV|    1.0|null|    true|I ordered a ladie...|07 30, 2012| AGBL3TTP6GV4X|\n",
      "|B00007GDFV|    4.0|   2|    true|Received the case...|03 12, 2010|A1Y36BSE9GKXLV|\n",
      "|B00007GDFV|    3.0|null|    true|Love it but it to...|10 31, 2017|A1L1U968VNYVA4|\n",
      "|B00007GDFV|    5.0|null|    true|I love it it is j...|10 24, 2017|A1NSKPSR0XZ0C9|\n",
      "|B00007GDFV|    5.0|null|   false|Quality product, ...|10 13, 2017|A3O5SXH5O8DWRP|\n",
      "|B00007GDFV|    4.0|null|   false|I like it except ...|10 11, 2017|A3I52T3ZCLRZZA|\n",
      "|B00007GDFV|    3.0|null|    true|It is smaller tha...| 10 7, 2017|A3VWTJR1QOI7JR|\n",
      "+----------+-------+----+--------+--------------------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_review.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a73a09-4b81-49a2-8778-6e164e526d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_review.write.parquet(\"/minipro/parquet/df_review.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37c2284-2ff5-4558-8d1c-a38c59223c07",
   "metadata": {},
   "source": [
    "# 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e47d8d-0b85-4e7c-bbb6-9e1e42efb7ec",
   "metadata": {},
   "source": [
    "# 저장한 자료 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2193ede6-d661-42ab-9461-dedec76c8c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# path_parquet = \"/minipro/parquet/df_review.parquet\"\n",
    "\n",
    "# df_review = spark.read.format(\"parquet\").load(path_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356bf515-9274-454d-9d80-8f37d9443539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_parquet2 = \"/minipro/parquet/df_product.parquet\"\n",
    "\n",
    "# df_product = spark.read.format(\"parquet\").load(path_parquet2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516c5569-7ab5-47a0-8765-bd464514207d",
   "metadata": {},
   "source": [
    "# 전체 로우 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "70faea6d-b300-4f10-8d3f-ce051c9c4631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df_review: 210445989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "row_count = df_review.count()\n",
    "print(\"Number of rows in df_review:\", row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e84ea-44b6-409f-b6a1-0db0ed3857a7",
   "metadata": {},
   "source": [
    "# asin을 기준으로 그룹화하고 리뷰 수 및 평균 평점 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f15dc820-4ba5-4489-9285-dde5d5e52fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:===============================================>        (11 + 2) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+\n",
      "|      asin|  cnt|avg_rate|\n",
      "+----------+-----+--------+\n",
      "|038568231X|58150|   4.041|\n",
      "|0297859382|44956|   3.891|\n",
      "|0007420412|44381|   4.491|\n",
      "|0141353678|37783|   4.682|\n",
      "|0312577222|36620|   4.784|\n",
      "|0099911701|33676|   4.591|\n",
      "|B000X1MX7E|32495|   4.053|\n",
      "|0553418025|30297|   4.632|\n",
      "|0007548672|27954|   4.553|\n",
      "|B00FLYWNYQ|27595|   4.591|\n",
      "|B000W5QSYA|26994|    4.46|\n",
      "|0316055433|25713|   3.758|\n",
      "|B00YSG2ZPA|24558|   4.913|\n",
      "|B00006CXSS|24489|   4.914|\n",
      "|B000WGWQG8|23584|   4.687|\n",
      "|0439023521|22538|   4.646|\n",
      "|8184776217|22511|   4.646|\n",
      "|0545582881|21603|   4.344|\n",
      "|B017WJ5PR4|21567|   4.715|\n",
      "|B017V4IPPO|21545|   4.715|\n",
      "+----------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, year, count, avg, round\n",
    "\n",
    "# reviewTime을 날짜 형식으로 변환\n",
    "df_review = df_review.withColumn(\"reviewTime\", to_date(\"reviewTime\", \"MM d, yyyy\"))\n",
    "# asin을 기준으로 그룹화하고 리뷰 수 및 평균 평점 계산\n",
    "review_stats = (\n",
    "    df_review\n",
    "    .groupBy(\"asin\")\n",
    "    .agg(\n",
    "        count(\"reviewerID\").alias(\"cnt\"),\n",
    "        round(avg(\"overall\"), 3).alias(\"avg_rate\")\n",
    "    )\n",
    "    .orderBy(\"cnt\", ascending=False)\n",
    ")\n",
    "# 결과 표시\n",
    "review_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9479216d-0d5b-45e8-861e-75e7937a875a",
   "metadata": {},
   "source": [
    "# avg_rate를 기준으로 DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca8d7c3b-6d5c-4461-8084-f6ba260c582d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:======================================================> (49 + 1) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+--------+--------------------+--------------------+--------------------+-------+--------+\n",
      "|      asin|cnt|avg_rate|               title|               brand|                rank|  price|category|\n",
      "+----------+---+--------+--------------------+--------------------+--------------------+-------+--------+\n",
      "|0006395848|  1|     5.0|Lesley Stowe Fine...|        Lesley Stowe|  344,063 in Books (|  $5.98|   Books|\n",
      "|0012066877|  1|     5.0|Making Math Meani...|         David Quine|3,400,087 in Books (|       |   Books|\n",
      "|0006481493|  4|     5.0|Leaving Home (Win...|Visit Amazon's Ga...|4,358,694 in Books (|       |   Books|\n",
      "|0006384889|  2|     5.0|The Story of My L...|Visit Amazon's Ch...|10,915,749 in Boo...|  $3.98|   Books|\n",
      "|0006754287|  3|     5.0|The Magician's Ne...|          C.S. Lewis|3,209,724 in Books (|$115.84|   Books|\n",
      "|000637512X|  1|     5.0|In the Sewers of ...|Visit Amazon's Ro...|5,854,557 in Books (| $10.17|   Books|\n",
      "|0006861482|  1|     5.0|The Walled Kingdo...|    Witold Rodzinski|8,611,460 in Books (|  $5.49|   Books|\n",
      "|0001944843|  1|     5.0|Little Treasury: ...|Visit Amazon's Hi...|16,371,336 in Boo...|  $7.27|   Books|\n",
      "|0007119607|  1|     5.0|The Times Food fo...|       C. J. Jackson|15,758,469 in Boo...| $10.05|   Books|\n",
      "|0002552515|  1|     5.0|           City Cats|  Jean-Claude Suares|5,574,616 in Books (|  $4.39|   Books|\n",
      "|0007121814|  1|     5.0|Mathematics (Coll...|Visit Amazon's Jo...|7,894,657 in Books (|       |   Books|\n",
      "|0002712229|  3|     5.0|         Anna, Soror|Visit Amazon's Ma...|7,676,731 in Books (|  $5.99|   Books|\n",
      "|0007142935|  1|     5.0|Realm of the Ring...|Visit Amazon's La...|1,799,144 in Books (| $11.76|   Books|\n",
      "|0003705544|  3|     5.0|BBC English Dicti...|       John Sinclair|3,305,265 in Books (| $37.02|   Books|\n",
      "|0007246765|  1|     5.0|Collins Arabic Ph...|Visit Amazon's Co...|4,748,929 in Books (|       |   Books|\n",
      "|0001942263|  2|     5.0|Little Grey Rabbi...|Visit Amazon's Al...|3,971,016 in Books (| $34.95|   Books|\n",
      "|0007262000|  1|     5.0|Serious Survival:...|Visit Amazon's Ma...|  491,514 in Books (|  $2.68|   Books|\n",
      "|000472500X|  4|     5.0|Scottish Country ...|Royal Scottish Co...|  958,784 in Books (|       |   Books|\n",
      "|0007277849|  2|     5.0|How to Master The...|Visit Amazon's Ti...|3,051,728 in Books (|  $6.93|   Books|\n",
      "|0005992400|  1|     5.0|Suffering: The Un...|       Frances Hogan|4,189,878 in Books (| $66.29|   Books|\n",
      "+----------+---+--------+--------------------+--------------------+--------------------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joined_df = review_stats.join(df_product, \"asin\")\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "sorted_df = joined_df.orderBy(col(\"avg_rate\").desc())\n",
    "sorted_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f192a026-c3ba-40ec-9755-e90af3e4dd76",
   "metadata": {},
   "source": [
    "# 카테고리별 시간에 따른 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a7eb0a56-c2eb-4ab8-b854-861715039b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 81:=====================================================>(963 + 6) / 980]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/26 17:32:06 WARN TaskSetManager: Lost task 840.0 in stage 81.0 (TID 13820) (datanode3 executor 5): java.io.FileNotFoundException: \n",
      "File does not exist: /minipro/ama/meta_Home_and_Kitchen.json\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:86)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:76)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:156)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2124)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:770)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:460)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n",
      "\n",
      "\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate\n",
      "the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\n",
      "recreating the Dataset/DataFrame involved.\n",
      "       \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:661)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/01/26 17:32:08 ERROR TaskSetManager: Task 840 in stage 81.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o577.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 840 in stage 81.0 failed 4 times, most recent failure: Lost task 840.3 in stage 81.0 (TID 13826) (datanode3 executor 5): java.io.FileNotFoundException: \nFile does not exist: /minipro/ama/meta_Home_and_Kitchen.json\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:86)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:76)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:156)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2124)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:770)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:460)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n\n\nIt is possible the underlying files have been updated. You can explicitly invalidate\nthe cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\nrecreating the Dataset/DataFrame involved.\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:661)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2604)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2603)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2603)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1178)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1178)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1178)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2798)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2787)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.io.FileNotFoundException: \nFile does not exist: /minipro/ama/meta_Home_and_Kitchen.json\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:86)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:76)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:156)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2124)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:770)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:460)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n\n\nIt is possible the underlying files have been updated. You can explicitly invalidate\nthe cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\nrecreating the Dataset/DataFrame involved.\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:661)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m joined_df \u001b[38;5;241m=\u001b[39m df_product\u001b[38;5;241m.\u001b[39mjoin(df_review, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124masin\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreviewYear\u001b[39m\u001b[38;5;124m'\u001b[39m, year(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreviewTime\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 카테고리와 리뷰 연도별로 그룹화하여 판매 수 집계\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m sales_by_category \u001b[38;5;241m=\u001b[39m \u001b[43mjoined_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreviewYear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43masin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_sales\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 각 카테고리에 대한 색상 정의\u001b[39;00m\n\u001b[1;32m     14\u001b[0m category_colors \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAmazon Fashion\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll Beauty\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVideo Games\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdarkseagreen\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     44\u001b[0m }\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/pandas/conversion.py:205\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    206\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    208\u001b[0m corrected_dtypes: List[Optional[Type]] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/dataframe.py:817\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \n\u001b[1;32m    809\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m--> 817\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o577.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 840 in stage 81.0 failed 4 times, most recent failure: Lost task 840.3 in stage 81.0 (TID 13826) (datanode3 executor 5): java.io.FileNotFoundException: \nFile does not exist: /minipro/ama/meta_Home_and_Kitchen.json\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:86)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:76)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:156)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2124)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:770)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:460)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n\n\nIt is possible the underlying files have been updated. You can explicitly invalidate\nthe cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\nrecreating the Dataset/DataFrame involved.\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:661)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2668)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2604)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2603)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2603)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1178)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1178)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1178)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2798)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2787)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.io.FileNotFoundException: \nFile does not exist: /minipro/ama/meta_Home_and_Kitchen.json\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:86)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:76)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:156)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2124)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:770)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:460)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n\n\nIt is possible the underlying files have been updated. You can explicitly invalidate\nthe cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\nrecreating the Dataset/DataFrame involved.\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:661)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:212)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:270)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/26 17:32:09 WARN TaskSetManager: Lost task 791.0 in stage 81.0 (TID 13818) (datanode2 executor 1): TaskKilled (Stage cancelled)\n",
      "24/01/26 17:32:09 WARN TaskSetManager: Lost task 717.0 in stage 81.0 (TID 13815) (datanode2 executor 1): TaskKilled (Stage cancelled)\n",
      "24/01/26 17:32:09 WARN TaskSetManager: Lost task 771.0 in stage 81.0 (TID 13817) (datanode5 executor 4): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 81:=====================================================>(963 + 3) / 980]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/26 17:32:09 WARN TaskSetManager: Lost task 864.3 in stage 81.0 (TID 13827) (datanode3 executor 5): TaskKilled (Stage cancelled)\n",
      "24/01/26 17:32:09 WARN TaskSetManager: Lost task 766.0 in stage 81.0 (TID 13816) (datanode3 executor 5): TaskKilled (Stage cancelled)\n",
      "24/01/26 17:32:09 WARN TaskSetManager: Lost task 614.0 in stage 81.0 (TID 13810) (datanode5 executor 4): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, to_date, count\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reviewTime을 올바른 날짜 형식으로 변환\n",
    "df_review = df_review.withColumn('reviewTime', to_date('reviewTime', 'MM dd, yyyy'))\n",
    "\n",
    "# asin을 기준으로 조인하고 reviewYear 컬럼 추가\n",
    "joined_df = df_product.join(df_review, 'asin').withColumn('reviewYear', year('reviewTime'))\n",
    "\n",
    "# 카테고리와 리뷰 연도별로 그룹화하여 판매 수 집계\n",
    "sales_by_category = joined_df.groupBy('category', 'reviewYear').agg(count('asin').alias('total_sales')).toPandas()\n",
    "\n",
    "# 각 카테고리에 대한 색상 정의\n",
    "category_colors = {\n",
    "    'Amazon Fashion': 'red',\n",
    "    'All Beauty': 'blue',\n",
    "    'Appliances': 'green',\n",
    "    'Arts Crafts and Sewing': 'purple',\n",
    "    'Automotive': 'orange',\n",
    "    'Books': 'brown',\n",
    "    'CDs and Vinyl': 'pink',\n",
    "    'Cell Phones and Accessories': 'gray',\n",
    "    'Clothing Shoes and Jewelry': 'cyan',\n",
    "    'Digital Music': 'magenta',\n",
    "    'Electronics': 'yellow',\n",
    "    'Gift Cards': 'lightgreen',\n",
    "    'Grocery and Gourmet Food': 'lightblue',\n",
    "    'Home and Kitchen': 'lightcoral',\n",
    "    'Industrial and Scientific': 'lightgray',\n",
    "    'Kindle Store': 'lime',\n",
    "    'Luxury Beauty': 'lavender',\n",
    "    'Magazine Subscriptions': 'gold',\n",
    "    'Movies and TV': 'darkred',\n",
    "    'Musical Instruments': 'darkblue',\n",
    "    'Office Products': 'darkgreen',\n",
    "    'Patio Lawn and Garden': 'darkorange',\n",
    "    'Pet Supplies': 'darkcyan',\n",
    "    'Prime Pantry': 'darkviolet',\n",
    "    'Software': 'darkkhaki',\n",
    "    'Sports and Outdoors': 'darkolivegreen',\n",
    "    'Tools and Home Improvement': 'darkorchid',\n",
    "    'Toys and Games': 'darksalmon',\n",
    "    'Video Games': 'darkseagreen'\n",
    "}\n",
    "# Matplotlib을 사용하여 그래프 그리기\n",
    "plt.figure(figsize=(12, 8))\n",
    "for category, color in category_colors.items():\n",
    "    category_data = sales_by_category[sales_by_category['category'] == category]\n",
    "    # x축과 y축 데이터를 리스트나 1차원 배열로 변환\n",
    "    x_data = category_data['reviewYear'].tolist()  # 또는 np.array(category_data['reviewYear'])\n",
    "    y_data = category_data['total_sales'].tolist()  # 또는 np.array(category_data['total_sales'])\n",
    "    plt.plot(x_data, y_data, marker='o', label=category, color=color)\n",
    "\n",
    "# 그래프 스타일 및 레이블 설정\n",
    "plt.title('Sales Over Years by Category')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.legend(title='Category')\n",
    "\n",
    "# 그래프 출력\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb62eeb-271e-40bc-890f-35640efd4f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cd479e-2558-45cc-a0cd-90a5f2f4aaac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "430e1269-0255-4147-b694-7356d40210cf",
   "metadata": {},
   "source": [
    "# 카테고리 별로 리뷰 수, 평균 별점 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae949a1-f174-4de3-9e68-860152a46c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, count\n",
    "\n",
    "# df_review와 df_product를 asin 컬럼을 기준으로 조인\n",
    "df_joined2 = df_review.join(df_product, \"asin\", \"inner\")\n",
    "\n",
    "# 카테고리별로 평균 overall, 평균 vote, reviewText의 개수 계산\n",
    "df_category_stats = df_joined2.groupBy(\"category\").agg(\n",
    "    avg(\"overall\").alias(\"avg_overall\"),\n",
    "    avg(\"vote\").alias(\"avg_vote\"),\n",
    "    count(\"reviewText\").alias(\"count_reviewText\")\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "df_category_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dff7e8-85c2-45d6-9f94-5335ba492cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfd0db6e-2ac6-4abe-8bbc-c7b35fd1e260",
   "metadata": {},
   "source": [
    "# 워드클라우드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce2763-bd84-4e2f-ba3a-37b5f641a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 리스트\n",
    "stop_words = set([\n",
    "    \"the\", \"is\", \"at\", \"which\", \"on\",\n",
    "    \"for\", \"with\", \"a\", \"an\", \"and\",\n",
    "    \"in\", \"it\", \"to\", \"has\", \"have\",\n",
    "    \"was\", \"were\", \"but\", \"if\", \"or\",\n",
    "    \"because\", \"as\", \"until\", \"while\",\n",
    "    \"of\", \"at\", \"by\", \"for\", \"with\",\n",
    "    \"about\", \"against\", \"between\", \"into\",\n",
    "    \"through\", \"during\", \"before\", \"after\",\n",
    "    \"above\", \"below\", \"to\", \"from\", \"up\",\n",
    "    \"down\", \"in\", \"out\", \"on\", \"off\",\n",
    "    \"over\", \"under\", \"again\", \"further\",\n",
    "    \"then\", \"once\", \"here\", \"there\",\n",
    "    \"when\", \"where\", \"why\", \"how\",\n",
    "    \"all\", \"any\", \"both\", \"each\",\n",
    "    \"few\", \"more\", \"most\", \"other\",\n",
    "    \"some\", \"such\", \"no\", \"nor\",\n",
    "    \"not\", \"only\", \"own\", \"same\",\n",
    "    \"so\", \"than\", \"too\", \"very\",\n",
    "     \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    "    \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\",\n",
    "    \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\",\n",
    "    \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\",\n",
    "    \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\",\n",
    "    \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\",\n",
    "    \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\",\n",
    "    \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\",\n",
    "    \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\", \"d\", \"ll\", \"m\", \"o\", \"re\", \"ve\",\n",
    "    \"y\", \"ain\", \"aren\", \"couldn\", \"didn\", \"doesn\", \"hadn\", \"hasn\", \"haven\", \"isn\", \"ma\", \"mightn\", \"mustn\", \"needn\",\n",
    "    \"shan\", \"shouldn\", \"wasn\", \"weren\", \"won\", \"wouldn\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa298e-1238-44ba-b30c-aca7e3c6785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import explode, col, lower, count\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# 워드 클라우드 생성 함수\n",
    "def generate_wordcloud(word_freq, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "# 배치 크기 설정\n",
    "batch_size = 1000000\n",
    "\n",
    "# 각 'overall' 평점에 대한 워드 클라우드 생성\n",
    "for overall in overall_values:\n",
    "    overall_rating = overall[\"overall\"]\n",
    "    df_filtered = df_review.filter(col(\"overall\") == overall_rating)\n",
    "\n",
    "    # 로우 번호를 추가하여 배치 처리 가능하게 함\n",
    "    windowSpec = Window.orderBy(monotonically_increasing_id())\n",
    "    df_filtered = df_filtered.withColumn(\"row_number\", row_number().over(windowSpec))\n",
    "\n",
    "    total_filtered_rows = df_filtered.count()\n",
    "    word_freq = {}\n",
    "\n",
    "    for i in range(0, total_filtered_rows, batch_size):\n",
    "        df_batch = df_filtered.filter(col(\"row_number\") > i).limit(batch_size)\n",
    "        df_batch = df_batch.drop(\"row_id\")\n",
    "        df_batch = df_batch.na.fill({\"reviewText\": \"\"}).withColumn(\"reviewText\", lower(col(\"reviewText\")))\n",
    "\n",
    "        tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"words\")\n",
    "        remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\", stopWords=list(stop_words))\n",
    "        words_df = tokenizer.transform(df_batch)\n",
    "        words_df_filtered = remover.transform(words_df)\n",
    "\n",
    "        word_counts = words_df_filtered.select(explode(col(\"filtered_words\")).alias(\"word\")).groupBy(\"word\").count()\n",
    "        word_counts_list = word_counts.collect()\n",
    "\n",
    "        for word_count in word_counts_list:\n",
    "            word = word_count[\"word\"]\n",
    "            count = word_count[\"count\"]\n",
    "            word_freq[word] = word_freq.get(word, 0) + count\n",
    "\n",
    "    generate_wordcloud(word_freq, f\"Word Cloud for Overall Rating: {overall_rating}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d863a6d2-03d7-49af-86a5-9033bc7705df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e837b-de25-4326-a60f-cd280d41aa63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
